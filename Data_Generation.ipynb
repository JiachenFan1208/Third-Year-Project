{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN/6DM5aFgnHdjQoJDuAvap"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"fcnaoHSu7DF4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"bbvxauH5B1Bi","executionInfo":{"status":"ok","timestamp":1679409867148,"user_tz":0,"elapsed":4084,"user":{"displayName":"Jiachen Fan","userId":"02096267599395372835"}}},"outputs":[],"source":["import nltk\n","from nltk import word_tokenize, sent_tokenize, pos_tag\n","from nltk.tokenize import RegexpTokenizer\n","import pandas as pd\n","import os\n","import re\n","from xml.dom import minidom\n","from xml.etree import cElementTree as ET\n","import csv\n","import random\n","import ast\n","from string import punctuation\n","from transformers import BartTokenizer"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.append('/content/drive/MyDrive/Third-Year-Project')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rUBRSWnq8sPn","executionInfo":{"status":"ok","timestamp":1679409890013,"user_tz":0,"elapsed":21142,"user":{"displayName":"Jiachen Fan","userId":"02096267599395372835"}},"outputId":"8c309595-7e8e-4fb6-ccf8-1142946a10bc"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')"],"metadata":{"id":"Ta8foeVfB70T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679409896201,"user_tz":0,"elapsed":1380,"user":{"displayName":"Jiachen Fan","userId":"02096267599395372835"}},"outputId":"a8f39bc7-b438-44ef-b07d-cbc3aee185b8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["def spans(txt):\n","  \"\"\"\n","  Seperate txext into words and returns a generator of (token, start, end) tuples for the given text.\n","  \n","  Input: txt - text to be tokenized\n","  Output: (token, start, end) tuples\n","  \"\"\"\n","  \n","  tokens=nltk.word_tokenize(txt)\n","\n","  offset = 0\n","  for token in tokens:\n","    offset = txt.find(token, offset)\n","    yield token, offset, offset+len(token)\n","    offset += len(token)\n","\n","def BIO_tagging(tags, text):\n","  \"\"\"\n","  This function tags the training dataset with BIO tags\n","  \n","  Input: a list of tags (the tags includs: 'Drugclass', 'Factor', 'Severity', 'AdverseReaction','Negation','Animal'), \n","      text content\n","  Output: A set of tuples which consist of words and BIO tag\n","  \"\"\"\n","  \n","  dict = {'DrugClass' : 'DUC', 'Factor' : 'FAC', 'Severity' : 'SVT', 'AdverseReaction' :'ADR', 'Negation' : 'NEG', 'Animal' : 'ANM'}\n","  \n","  tagged_words = []\n","  tagged_position = []\n","\n","\n","  tokens = spans(text)\n","  \n","  for token in tokens:\n","\n","    tagged = False\n","\n","    letter = [token[0][i] for i in range(0,len(token[0]))]\n","    letter_off = [i for i in range(token[1],token[2])]\n","\n","\n","    if len(letter) != len(letter_off):\n","      raise ValueError('letter and offset does not match.')\n","\n","    punc = [letter.index(i) for i in letter if i in punctuation]\n","    no_punc = [j for j in letter_off if letter_off.index(j) not in punc]\n","    \n","\n","    if len(no_punc) != 0:\n","      temp = list(token)\n","      temp[1] = min(no_punc)\n","      temp[2] = max(no_punc)\n","      token = tuple(temp)\n","\n","\n","    for i in range(len(tags)):\n","\n","      pos = tags[i].attrib['start'].split(',')\n","      length = tags[i].attrib['len'].split(',')\n","\n","      if len(pos) == 1:\n","          \n","        d_pos = int(pos[0])\n","        d_length = int(length[0])\n","\n","        if token[1] != d_pos and (d_pos >= token[1] or (d_pos+d_length) <= token[1]):\n","          continue      \n","        else:                  \n","          if token[1] == d_pos :\n","            B_tag = (token[0].strip(),'B-'+ dict[tags[i].attrib['type']])\n","            tagged_words.append(B_tag)\n","            tagged = True\n","            \n","              \n","          elif d_pos < token[1] < d_pos+d_length:\n","            I_tag = (token[0].strip(),'I-'+ dict[tags[i].attrib['type']])\n","            tagged_words.append(I_tag)\n","            tagged = True\n","                \n","      else:\n","\n","        for m in range(len(pos)):\n","\n","          c_pos = int(pos[m])\n","          c_length = int(length[m])\n","\n","          if token[1] != c_pos and (c_pos >= token[1] or (c_pos+c_length) <= token[1]):\n","            continue            \n","          else:\n","              \n","            if token[1] == c_pos and m == 0:\n","              B_tag = (token[0].strip(),'B-'+ dict[tags[i].attrib['type']])\n","              tagged_words.append(B_tag)\n","              tagged = True\n","              break\n","            \n","            elif token[1] == c_pos and m != 0:\n","              I_tag = (token[0].strip(),'I-'+ dict[tags[i].attrib['type']])\n","              tagged_words.append(I_tag)\n","              tagged = True\n","              break\n","                \n","            elif c_pos < token[1] < c_pos+c_length:\n","              I_tag = (token[0].strip(),'I-'+ dict[tags[i].attrib['type']])\n","              tagged_words.append(I_tag)\n","              tagged = True\n","              break\n","                  \n","      if tagged == True:\n","          break\n","\n","    if tagged == False:\n","      O_tag = (token[0].strip(),'O')\n","      tagged_words.append(O_tag)\n","\n","  # Just leave the ADR tag, set all other tags to 'O'\n","  binary_classify = []\n","  for tag in tagged_words:\n","    if tag[1] != 'B-ADR' and tag[1] != 'I-ADR':\n","      temp = list(tag)\n","      temp[1] = 'O'\n","      binary_classify.append(tuple(temp))\n","    else:\n","      binary_classify.append(tag)\n","\n","\n","  return binary_classify\n","\n","\n","\n","def  preprocess(string):\n","  \"\"\"\n","  Preprocessing for all datasets\n","  \n","  Input: A sentence from original text\n","  Output: Preprocessed sentence\n","  \"\"\"\n","  \n","  string = re.sub(r\"[^\\w(),|!?\\'\\`\\:\\-\\.;\\$%#]\", \" \", string)\n","  string = re.sub(r\"\\'s\", \" is\", string)\n","  string = re.sub(r\"\\'ve\", \" have\", string)\n","  string = re.sub(r\"n\\'t\", \" not\", string)\n","  string = re.sub(r\"\\'re\", \" are\", string)\n","  string = re.sub(r\"\\'d\", \" would\", string)\n","  string = re.sub(r\"\\'ll\", \" will\", string)\n","  string = re.sub(r\"(?<=\\w)\\.\\.\\.\", \" ... \", string)\n","  string = re.sub(r\"(?<=\\w)\\.\", \" . \", string)\n","  string = re.sub(r\"(?<=\\w),\", \" , \", string)\n","  string = re.sub(r\"(?<=\\w);\", \" ; \", string)\n","  string = re.sub(r\"(?<=\\w)!\", \" ! \", string)\n","  string = re.sub(r\"\\((?=\\w)\", \" ( \", string)\n","  string = re.sub(r\"(?<=\\w)\\)\", \" ) \", string)\n","  string = re.sub(r\"(?<=\\w)\\?\", \" ? \", string)\n","  string = re.sub(r\"\\s{2,}\", \" \", string)\n","  \n","  \n","  return string\n","\n","\n","\n","\n","def POS_tagging(text):\n","  \"\"\"\n","  \n","  This function tags the training dataset with POS tags\n","  \n","  Input: text - text to be tokenized\n","  Output: A set of tuples which consist of words and POS tag\n","  \"\"\"\n","\n","  #Tokenization\n","  tokens = word_tokenize(text)\n","\n","  # Part-of-speech Tagging\n","  pos_tagged = pos_tag(tokens)\n","      \n","  return pos_tagged"],"metadata":{"id":"4Bra3wauB_vT","executionInfo":{"status":"ok","timestamp":1679409902689,"user_tz":0,"elapsed":293,"user":{"displayName":"Jiachen Fan","userId":"02096267599395372835"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class Data_generation():\n","  \"\"\"\n","  This class do some preprocess to the original ADR sample corpus, \n","  and transform data for training with the format: sentence + template.\n","  \n","  \"\"\"\n","\n","  def __init__(self, path):\n","    self.path = path\n","    self.text_content_tags = {} # Format: {text: [tags]}\n","    self.tree = ET.parse(path)\n","    self.root = self.tree.getroot()\n","    self.POS_tags = {} # Sentence is the key, POS tags are the values, same as BIO tags and template\n","    self.BIO_tags = {}\n","    self.template = {}\n","\n","\n","  def set_text_content_tags(self):\n","    \"\"\"\n","    This function extracts the text content and tags from the original corpus\n","    \"\"\"\n","\n","    for section in self.root[0].findall('Section'):\n","      tags = []\n","      for mention in self.root[1].findall('Mention'):\n","        if mention.attrib['section'] == section.attrib['id']:\n","          tags.append(mention)\n","        self.text_content_tags[section.text] = tags\n","\n","  \n","\n","  def sentence_divide(self,text):\n","    \"\"\"\n","    This function divides the text into sentences\n","\n","    Input: Text\n","    Output: A list of sentences\n","    \"\"\"\n","    tokens = nltk.sent_tokenize(text)\n","\n","    sentences = []\n","\n","    offset = 0\n","    for token in tokens:\n","      offset = text.find(token, offset)\n","      yield token, offset, offset+len(token)\n","      sentences.append((token, offset, offset+len(token)))\n","      offset += len(token)\n","\n","    return sentences\n","\n","  \n","\n","  def data_preprocess(self):\n","    \"\"\"\n","    This function preprocesses the original corpus and transform data for training\n","    \"\"\"\n","\n","    for key, value in self.text_content_tags.items():\n","\n","      text = key\n","      tags = value\n","      \n","      # Divide text into sentences\n","      sentences = self.sentence_divide(text)\n","\n","      # Generate BIO tags and POS tags for the text\n","      BIO_tags = BIO_tagging(tags, text)\n","      POS_tags = POS_tagging(text)\n","\n","      word_count = 0\n","\n","      for sentence in list(sentences):\n","\n","        words = nltk.word_tokenize(sentence[0])\n","\n","        # Map the BIO tags and POS tags to the specific sentence\n","        self.BIO_tags[sentence] = BIO_tags[word_count:word_count+len(words)]\n","        self.POS_tags[sentence] = POS_tags[word_count:word_count+len(words)]\n","\n","        # Get the start and end index of the sentence\n","        sen_start = sentence[1]\n","        sen_end = sentence[2]\n","\n","        templates = []\n","\n","        # Generate templates for each sentence\n","        for tag in tags:\n","\n","          pos = tag.attrib['start'].split(',')\n","          length = tag.attrib['len'].split(',')\n","\n","          # If the words of the annotated phrase are connected in the sentence\n","          if len(pos) == 1:\n","\n","            # get the start index and the length of the annotated words      \n","            c_pos = int(pos[0])\n","            c_length = int(length[0])\n","\n","            if sen_start <= c_pos and sen_end >= c_pos+c_length:\n","\n","              if tag.attrib['type'] == 'AdverseReaction':\n","\n","                templates.append(tag.attrib['str'] + \" is an adverse reaction entity\")\n","\n","          # If the words of the annotated phrase are not connected in the sentence     \n","          else:\n","\n","            # Get the start index and length for the first word in the phrase\n","            b_pos = int(pos[0])\n","            b_length = int(length[0])\n","\n","            # Get the start index and length for the last word in the phrase\n","            e_pos = int(pos[-1])\n","            e_length = int(length[-1])\n","\n","            if sen_start <= b_pos and sen_end >= e_pos+e_length:\n","\n","              if tag.attrib['type'] == 'AdverseReaction':\n","  \n","                templates.append(tag.attrib['str'] + \" is an adverse reaction entity\")\n","\n","        self.template[sentence] = templates\n","\n","        word_count += len(words)\n","\n","      # Delete the sentences that do not have any entity\n","      for key in list(self.template.keys()):\n","        if len(self.template[key]) == 0:\n","          del self.template[key]\n","          del self.BIO_tags[key]\n","          del self.POS_tags[key]\n","\n","          \n","\n","  def no_entity_generation(self, sample, num_template):\n","    \"\"\"\n","    This function generates negative samples for training\n","    \"\"\"\n","    no_entity = []\n","\n","    BIO_tag = self.BIO_tags[sample]\n"," \n","    num_no = num_template # Generate same times of the number of positive samples\n","\n","    for k in range(0, num_no):\n","\n","      Generated =  False\n","\n","      while Generated == False:\n","\n","        # Randomly generate n-grams, n is between 1 and 3\n","        num_n_grams = random.randint(1,3)\n","\n","        # Randomly generate the start position of the n-grams\n","        start_of_n_grams = random.choice(range(0, len(BIO_tag)-num_n_grams))\n","\n","        if ' '.join(BIO_tag[i][0] for i in range(start_of_n_grams, start_of_n_grams + num_n_grams))  in no_entity:\n","          continue\n","\n","        if BIO_tag[start_of_n_grams][1] == 'B-ADR' and ('O' not in [BIO_tag[i][1] for i in range(start_of_n_grams+1, start_of_n_grams + num_n_grams)] \n","                                                   and 'B-ADR' not in [BIO_tag[j][1] for j in range(start_of_n_grams+1, start_of_n_grams + num_n_grams)]):\n","          continue\n","        \n","        temp = \"\"\n","\n","        for j in range(start_of_n_grams, start_of_n_grams+num_n_grams):\n","\n","          temp += BIO_tag[j][0] + \" \"\n","\n","          if j == start_of_n_grams+num_n_grams-1:\n","            temp = temp.replace('\\n', ' ')\n","            temp = temp.strip()\n","            no_entity.append(temp+\" is not a named entity\")\n","            Generated = True  \n","            break\n","          \n","        \n","\n","    return no_entity\n"],"metadata":{"id":"7bogIQWoCAz7","executionInfo":{"status":"ok","timestamp":1679413569200,"user_tz":0,"elapsed":206,"user":{"displayName":"Jiachen Fan","userId":"02096267599395372835"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["path = 'drive/MyDrive/Third-Year-Project/data/train_xml'# Path which contains xml files\n","\n","train_path = 'drive/MyDrive/Third-Year-Project/data/train.csv'\n","dev_path = 'drive/MyDrive/Third-Year-Project/data/dev.csv'\n","test_path = 'drive/MyDrive/Third-Year-Project/data/test.csv'\n","\n","count = 0\n","\n","if os.path.exists(train_path):\n","\n","    os.remove(train_path)\n","\n","if os.path.exists(dev_path):\n","\n","    os.remove(dev_path)\n","\n","if os.path.exists(test_path):\n","\n","    os.remove(test_path)\n","\n","\n","with open(train_path, 'w', newline='') as csvfile1:\n","\n","  with open(dev_path, 'w', newline='') as csvfile2:\n","\n","    with open(test_path, 'w', newline='') as csvfile3:\n","\n","\n","      sen_tem_train = []\n","      sen_tem_test = []\n","      sen_tem_valid = []\n","\n","      for file in os.listdir(path):\n","\n","          file_path = os.path.join(path, file)\n","\n","          example = Data_generation(file_path)\n","\n","          example.set_text_content_tags()\n","\n","          example.data_preprocess()\n","\n","          writer_train = csv.writer(csvfile1)\n","          writer_dev = csv.writer(csvfile2)\n","          writer_test = csv.writer(csvfile3)\n","\n","          sen_tem = []         \n","          for key,value in example.template.items():\n","\n","              # Get sentence that length <= 500 and >= 80.\n","              if len(value) != 0 and len(key[0]) <= 800 and len(key[0]) >= 80:\n","\n","                  sen_tem.append((key,value))\n","\n","                  \n","\n","          temp_train_valid = random.sample(sen_tem,round(len(sen_tem)*0.9))\n","          temp_test = [i for i in sen_tem if i not in temp_train_valid]\n","\n","          temp_train = random.sample(temp_train_valid, round(len(temp_train_valid)*0.75))\n","          temp_valid = [j for j in temp_train_valid if j not in temp_train]\n","\n","          for content in temp_test:\n","            for i in range(len(content[1])):\n","\n","              text = content[0][0].replace('\\n', ' ')\n","              text = text.strip()\n","\n","              sen_tem_test.append((text, content[1][i], example.BIO_tags[content[0]]))\n","              count += 1\n","\n","\n","          for content in temp_valid:\n","            for i in range(len(content[1])):\n","\n","              text = content[0][0].replace('\\n', ' ')\n","              text = text.strip()\n","\n","              sen_tem_valid.append((text, content[1][i]))\n","              count += 1\n","\n","\n","          for content in temp_train:\n","            batch_set = []\n","\n","            text = content[0][0].replace('\\n', ' ')\n","            text = text.strip()\n","\n","            for i in range(len(content[1])):\n","              batch_set.append((text,content[1][i]))\n","              count += 1\n","            \n","            # Generate nagative examples for traing set, with ratio: pos:neg = 1:1\n","            no_entity = example.no_entity_generation(content[0], len(content[1]))\n","\n","            for i in range(len(no_entity)):\n","                batch_set.append((text,no_entity[i]))\n","                count += 1\n","            \n","            # disrupt the order of pos and neg examples\n","            random.shuffle(batch_set)\n","\n","            for i in batch_set:\n","              sen_tem_train.append(i)\n","\n","\n","      for content in sen_tem_train:\n","          writer_train.writerow([content[0], content[1]])\n","\n","      for content in sen_tem_valid:\n","          writer_dev.writerow([content[0], content[1]])\n","\n","      for content in sen_tem_test:\n","          writer_test.writerow([content[0], content[1], content[2]])\n","\n","      print(count, end = '\\r')\n","\n","\n","df1 = pd.read_csv(train_path, header=None)\n","df2 = pd.read_csv(dev_path, header=None)\n","df3 = pd.read_csv(test_path, header=None)\n","print('Train data generated: ', len(df1))      \n","print('Dev data generated: ', len(df2))   \n","print('Test data generated: ', len(df3))  \n","\n","print('Generated :' , count)\n"],"metadata":{"id":"_s2k9ResCC96","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679413629904,"user_tz":0,"elapsed":52576,"user":{"displayName":"Jiachen Fan","userId":"02096267599395372835"}},"outputId":"665077d7-143c-4eb2-b5ac-61f5ceefca4b"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["15236\rTrain data generated:  12184\n","Dev data generated:  2129\n","Test data generated:  923\n","Generated : 15236\n"]}]}]}